{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Problem 1.}$ Run the original word2vec.exe and create embeddings from text8.zip file as\n",
    "described in lecture notes. Demonstrate that demo-word.sh works by finding similar word\n",
    "to two of words of your choice.\n",
    "(15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Steps and Code:}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Run demo-word.sh. It downloads text8.zip from http://mattmahoney.net/dc/text8.zip and runs word2vec.exe. 'word2vec.exe' runs the continuous-bag-of words model with dimensionality of 200, the size of the context window of 8, using negative sampling algorithm with threshold of 1e-4 to produce embeddings from the corpus of words in text8.zip. The embeddings are saved in file vectors.bin."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ ./demo-word.sh\n",
    "make: Nothing to be done for 'all'.\n",
    "---------------------------------------------------------------------------------------------------\n",
    "Note that for the word analogy to perform well, the model should be trained on much larger data set\n",
    "Example input: paris france berlin\n",
    "---------------------------------------------------------------------------------------------------\n",
    "Starting training using file text8\n",
    "Vocab size: 71291\n",
    "Words in train file: 16718843\n",
    "Alpha: 0.000005  Progress: 100.10%  Words/thread/sec: 83.93k\n",
    "real    12m50.744s\n",
    "user    49m59.656s\n",
    "sys     0m2.140s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Call distance.exe with 'vectors.bin' as the argument. This script outputs the most similar words from the corpus based on cosine distance. I have used the words 'sincere' and 'politics' for my demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ ./distance vectors.bin<br>\n",
    "Enter word or sentence (EXIT to break): sincere\n",
    "\n",
    "Word: sincere  Position in vocabulary: 16291\n",
    "\n",
    "                                              Word       Cosine distance\n",
    "------------------------------------------------------------------------\n",
    "                                          believer              0.508035\n",
    "                                        conscience              0.484672\n",
    "                                             faith              0.471297\n",
    "                                         salvation              0.470599\n",
    "                                        repentance              0.463140\n",
    "                                       selfishness              0.454214\n",
    "                                         rejection              0.446344\n",
    "                                            unduly              0.442633\n",
    "                                             piety              0.442080\n",
    "                                    predestination              0.441922\n",
    "                                           provoke              0.440755\n",
    "                                           worldly              0.437072\n",
    "                                            virtue              0.436129\n",
    "                                          kindness              0.435065\n",
    "                                       forgiveness              0.434058\n",
    "                                         brotherly              0.429010\n",
    "                                           honesty              0.425954\n",
    "                                         sincerity              0.425922\n",
    "                                             truth              0.421282\n",
    "                                        expressing              0.421233\n",
    "                                           despair              0.420694\n",
    "                                     righteousness              0.419236\n",
    "                                       affirmation              0.417097\n",
    "                                          devotion              0.415489\n",
    "                                           desires              0.414598\n",
    "                                          humility              0.413941\n",
    "                                           virtues              0.413085\n",
    "                                        compassion              0.412717\n",
    "                                       benevolence              0.411440\n",
    "                                             grace              0.409755\n",
    "                                          feelings              0.407946\n",
    "                                            hatred              0.404597\n",
    "                                           wronged              0.403363\n",
    "                                       realisation              0.402651\n",
    "                                          salvific              0.402290\n",
    "                                        absolution              0.400742\n",
    "                                             guilt              0.400522\n",
    "                                          sympathy              0.400306\n",
    "                                         justified              0.399118\n",
    "                                           impious              0.398012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter word or sentence (EXIT to break): politics\n",
    "\n",
    "Word: politics  Position in vocabulary: 823\n",
    "\n",
    "                                              Word       Cosine distance\n",
    "------------------------------------------------------------------------\n",
    "                                         political              0.657083\n",
    "                                  multiculturalism              0.498159\n",
    "                                        radicalism              0.482741\n",
    "                                 constitutionalism              0.473870\n",
    "                                          ideology              0.469087\n",
    "                                         sociology              0.466884\n",
    "                                           liberal              0.461836\n",
    "                                      jeffersonian              0.453224\n",
    "                                        multiparty              0.451197\n",
    "                                        democratic              0.450226\n",
    "                                           affairs              0.440730\n",
    "                                         democracy              0.440405\n",
    "                                     republicanism              0.434517\n",
    "                                          religion              0.432421\n",
    "                                      expansionism              0.430243\n",
    "                                            policy              0.429431\n",
    "                                       nationalism              0.428052\n",
    "                                   democratisation              0.421833\n",
    "                                         reformist              0.420629\n",
    "                                        federalist              0.419108\n",
    "                                             party              0.416572\n",
    "                                        opposition              0.411188\n",
    "                                           leftist              0.410280\n",
    "                                              whig              0.407559\n",
    "                                       colonialism              0.405823\n",
    "                                        ecuadorean              0.402198\n",
    "                                        jacksonian              0.400372\n",
    "                                        government              0.399792\n",
    "                                          centrist              0.399504\n",
    "                                          leanings              0.399229\n",
    "                                        figurehead              0.398373\n",
    "                                           economy              0.398162\n",
    "                                  internationalism              0.397347\n",
    "                                       realpolitik              0.397047\n",
    "                                           dissent              0.393553\n",
    "                                  environmentalism              0.393414\n",
    "                                        guatemalan              0.392742\n",
    "                                        separatism              0.389713\n",
    "                                  authoritarianism              0.387868\n",
    "                                          activism              0.387842\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Problem 2.}$ Use word-analogy.sh script to rectify error I made on slide 43 of the lecture\n",
    "notes. I asked for analogy amongst words: king man woman. I should have asked for\n",
    "words: man king woman. Does the result makes sense and is it in agreement with the\n",
    "vector similarities we advertised.\n",
    "(20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Steps and Code:}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Call 'word-analogy.exe' with 'vectors.bin' as argument. This allows us to generate word analogies from vectors.bin file generated in previous problem. \n",
    "2\\. Enter 'man king woman' to get the closest matching words for the analogy 'man:king::woman:?'. We find that 'queen' comes out at top, which is expected.\n",
    "This result is in agreement with the vector similarities described in the lecture."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ ./word-analogy vectors.bin\n",
    "Enter three words (EXIT to break): man king woman\n",
    "\n",
    "Word: man  Position in vocabulary: 243\n",
    "\n",
    "Word: king  Position in vocabulary: 187\n",
    "\n",
    "Word: woman  Position in vocabulary: 1012\n",
    "\n",
    "                                              Word              Distance\n",
    "------------------------------------------------------------------------\n",
    "                                             queen              0.584235\n",
    "                                        montferrat              0.538089\n",
    "                                             anjou              0.502499\n",
    "                                           heiress              0.490614\n",
    "                                           marries              0.486458\n",
    "                                          daughter              0.486101\n",
    "                                         betrothed              0.480538\n",
    "                                           infanta              0.477006\n",
    "                                          isabella              0.472338\n",
    "                                           sibylla              0.470298\n",
    "                                               vii              0.467301\n",
    "                                               son              0.458748\n",
    "                                           matilda              0.457915\n",
    "                                          burgundy              0.457301\n",
    "                                           yolande              0.455971\n",
    "                                            throne              0.454726\n",
    "                                            prince              0.450469\n",
    "                                           consort              0.449910\n",
    "                                         melisende              0.449685\n",
    "                                         elizabeth              0.445980\n",
    "                                            aragon              0.445977\n",
    "                                           eustace              0.441950\n",
    "                                         aquitaine              0.435223\n",
    "                                            valois              0.432736\n",
    "                                         sigismund              0.431564\n",
    "                                           gascony              0.430900\n",
    "                                              wife              0.428418\n",
    "                                            urraca              0.427811\n",
    "                                           brienne              0.427661\n",
    "                                            wedded              0.427367\n",
    "                                            barons              0.424761\n",
    "                                          claimant              0.422896\n",
    "                                         ahasuerus              0.422823\n",
    "                                             savoy              0.422737\n",
    "                                            regent              0.420663\n",
    "                                          sobieski              0.420356\n",
    "                                        vermandois              0.419769\n",
    "                                             sahle              0.419284\n",
    "                                          philippa              0.418436\n",
    "                                              heir              0.418333\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Problem 3.}$ Use demo-phrases.sh script to embed two-word phrases. Once the\n",
    "embedding is done, examine the most similar phrases to two phrases: london_bridge\n",
    "and red_wine.\n",
    "(20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Steps and Code:}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\.Run 'demo-phrases.sh' to train the model with a corpus of phrases."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ ./demo-phrases.sh\n",
    "make: Nothing to be done for 'all'.\n",
    "--2018-04-07 23:47:53--  http://www.statmt.org/wmt14/training-monolingual-news-c                                                                                            rawl/news.2012.en.shuffled.gz\n",
    "Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\n",
    "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:80... connected.\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 786717767 (750M) [application/x-gzip]\n",
    "Saving to: ‘news.2012.en.shuffled.gz’\n",
    "\n",
    "news.2012.en.shuffl 100%[===================>] 750.27M  4.45MB/s    in 4m 59s\n",
    "\n",
    "2018-04-07 23:52:52 (2.51 MB/s) - ‘news.2012.en.shuffled.gz’ saved [786717767/78                                                                                            6717767]\n",
    "\n",
    "Starting training using file news.2012.en.shuffled-norm0\n",
    "Words processed: 296900K     Vocab size: 33198K\n",
    "Vocab size (unigrams + bigrams): 18838711\n",
    "Words in train file: 296901342\n",
    "Words written: 296900K\n",
    "real    37m7.760s\n",
    "user    23m23.453s\n",
    "sys     12m34.062s\n",
    "Starting training using file news.2012.en.shuffled-norm0-phrase0\n",
    "Words processed: 280500K     Vocab size: 38761K\n",
    "Vocab size (unigrams + bigrams): 21728781\n",
    "Words in train file: 280513979\n",
    "Words written: 280500K\n",
    "real    37m30.823s\n",
    "user    22m50.890s\n",
    "sys     13m42.546s\n",
    "Starting training using file news.2012.en.shuffled-norm1-phrase1\n",
    "Vocab size: 681320\n",
    "Words in train file: 283545447\n",
    "Alpha: 0.000005  Progress: 100.00%  Words/thread/sec: 124.39k\n",
    "real    439m0.396s\n",
    "user    574m40.187s\n",
    "sys     1m20.453s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Enter 'london_bridge' to get the most similar words/phrases by cosine distance. We find 'liverpool_street', 'thames' and 'underground_station' at top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter word or sentence (EXIT to break): london_bridge\n",
    "\n",
    "Word: london_bridge  Position in vocabulary: 44297\n",
    "\n",
    "                                              Word       Cosine distance\n",
    "------------------------------------------------------------------------\n",
    "                                  liverpool_street              0.637184\n",
    "                                            thames              0.611050\n",
    "                               underground_station              0.606648\n",
    "                                      king's_cross              0.600436\n",
    "                                      tower_bridge              0.592890\n",
    "                                       kings_cross              0.579351\n",
    "                                    central_london              0.573142\n",
    "                                      jubilee_line              0.564611\n",
    "                                st_pancras_station              0.563033\n",
    "                                     cannon_street              0.556554\n",
    "                                      tube_station              0.556343\n",
    "                                           aldgate              0.555779\n",
    "                                      river_thames              0.554985\n",
    "                                  trafalgar_square              0.546115\n",
    "                           docklands_light_railway              0.544713\n",
    "                                         southwark              0.540538\n",
    "                                     charing_cross              0.533210\n",
    "                                       marble_arch              0.532528\n",
    "                                 piccadilly_circus              0.530807\n",
    "                                london_underground              0.529245\n",
    "                                          moorgate              0.526509\n",
    "                          st_pancras_international              0.525059\n",
    "                                       city_centre              0.524259\n",
    "                                           holborn              0.523520\n",
    "                               victoria_embankment              0.522622\n",
    "                                      central_line              0.522478\n",
    "                                  battersea_bridge              0.520968\n",
    "                                      olympic_park              0.515492\n",
    "                                westminster_bridge              0.515126\n",
    "                                    olympic_venues              0.514914\n",
    "                                     tube_stations              0.511410\n",
    "                                         crossrail              0.510702\n",
    "                                      canary_wharf              0.509836\n",
    "                                    russell_square              0.507313\n",
    "                                 hungerford_bridge              0.504282\n",
    "                                   old_vic_tunnels              0.503995\n",
    "                                        london_eye              0.503620\n",
    "                                     chancery_lane              0.502560\n",
    "                                          bakerloo              0.496537\n",
    "                                              tram              0.496445"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Enter 'red-wine' to get the most similar words/phrases by cosine distance. We find 'white_wine', 'wine' and 'astringents' at top, which makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter word or sentence (EXIT to break): red_wine\n",
    "\n",
    "Word: red_wine  Position in vocabulary: 17150\n",
    "\n",
    "                                              Word       Cosine distance\n",
    "------------------------------------------------------------------------\n",
    "                                        white_wine              0.672334\n",
    "                                              wine              0.600739\n",
    "                                       astringents              0.578001\n",
    "                                             drink              0.565842\n",
    "                                            lillet              0.564723\n",
    "                                     pint_ml_stock              0.562248\n",
    "                                           cup_dry              0.560049\n",
    "                                       tonic_water              0.559200\n",
    "                                       grape_juice              0.553419\n",
    "                                   cranberry_juice              0.551464\n",
    "                                  antioxidant_rich              0.550222\n",
    "                                         red_meats              0.547388\n",
    "                             contains_antioxidants              0.546023\n",
    "                                 drinking_red_wine              0.545326\n",
    "                                       stroke_risk              0.544932\n",
    "                                       resveratrol              0.544157\n",
    "                                          sloe_gin              0.542655\n",
    "                                          digestif              0.540328\n",
    "                                            prunes              0.539998\n",
    "                                       steak_sauce              0.537426\n",
    "                                            aperol              0.530161\n",
    "                                blood_orange_juice              0.529423\n",
    "                                      soft_tannins              0.529031\n",
    "                                            bottle              0.528744\n",
    "                                          prosecco              0.527103\n",
    "                                          calvados              0.525403\n",
    "                                          muscadet              0.525041\n",
    "                                         red_wines              0.524408\n",
    "                                         green_tea              0.524231\n",
    "                                     dessert_wines              0.522405\n",
    "                                           snifter              0.520978\n",
    "                                          highball              0.520755\n",
    "                                  tablespoons_rice              0.520585\n",
    "                                         warm_milk              0.517657\n",
    "                                  balsamic_vinegar              0.517653\n",
    "                                          aperitif              0.517180\n",
    "                                    fortified_wine              0.516745\n",
    "                                    pink_champagne              0.514472\n",
    "                                      dessert_wine              0.514456\n",
    "                                   juniper_berries              0.514000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Problem 4}$ Use the following Gensim code to transform binary file vectors.bin into a text file, vectors.txt.\n",
    "```python\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('vectors.bin', binary=True)\n",
    "# C text format\n",
    "model.save_word2vec_format('vectors.txt', binary=False)\n",
    "```\n",
    "Tell us what is the volume of new file when compared with the C binary file\n",
    "vectors.bin. Examine the file and locate vectors for words: queen, king, woman\n",
    "and man. Present for us the first 10 coordinates of each vector.\n",
    "(20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Steps and Code:}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Use the code to produce 'vectors.txt' from 'vectors.bin' produced from Problem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-08 00:06:55,655 : INFO : loading projection weights from vectors.bin\n",
      "2018-04-08 00:06:57,149 : INFO : loaded (71291, 200) matrix from vectors.bin\n",
      "2018-04-08 00:06:57,213 : INFO : storing 71291x200 projection weights into vectors.txt\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('vectors.bin', binary=True)\n",
    "# C text format\n",
    "model.save_word2vec_format('vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. We find that vectors.txt is three times the size of vectors.txt at 159 MB and 57 MB respectively. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ ls -al\n",
    "-rwx------+ 1 asinha         asinha  57704809 Apr  7 21:50 vectors.bin\n",
    "-rwxrwx---+ 1 Administrators asinha 159229581 Apr  8 00:07 vectors.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Load the newly created 'vectors.txt' file with 'load-word2vec' command. Use 'get_vector' command to get the cosine distances of the words 'king', 'queen', 'man' and 'woman'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-08 00:08:07,448 : INFO : loading projection weights from vectors.txt\n",
      "2018-04-08 00:08:26,278 : INFO : loaded (71291, 200) matrix from vectors.txt\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format('vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6221384  -2.381173   -0.62738276 -1.0975784   1.2373395  -3.1950328\n",
      " -0.74022526  0.47273895 -1.1066034  -1.5091859 ]\n",
      "[-1.8954808  -0.8628662  -2.0278444  -0.5545433   2.951458   -1.4985704\n",
      " -2.6483643  -2.0140924  -0.97132504 -1.605231  ]\n",
      "[-0.58591014 -1.0758336  -1.1054654  -2.7096047   0.16424485  0.6074105\n",
      " -0.13503285  2.3363764   3.0286949  -1.4967729 ]\n",
      "[ 1.4167926  -0.7298542  -2.2284133  -1.2658012   1.0197504  -0.02355143\n",
      "  1.0337201   0.6177318   1.4861876   0.45655224]\n"
     ]
    }
   ],
   "source": [
    "print(model.get_vector('queen')[0:10])\n",
    "print(model.get_vector('king')[0:10])\n",
    "print(model.get_vector('woman')[0:10])\n",
    "print(model.get_vector('man')[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Problem 5}$  Run the code in Embeddings section of the provided Jupyter notebook.\n",
    "Notice that the embeddings (vector coordinates) get deposited in Numpy array called\n",
    "final_embeddings. Tell us what are the dimensions of that array? Could you create\n",
    "a panda data frame which would, besides those embeddings, also contain words\n",
    "corresponding to vectors in final_embeddings array. Export that panda as a .txt file\n",
    "or a .scv file and compare vectors in exported file with vectors for words; king, queen,\n",
    "man and woman in file vectors.txt you created in the previous problem\n",
    "(25%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Code and Steps:}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Run the embeddings section of the code as provided in the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.show()\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = r\"/home/ec2-user/Assignment/\"\n",
    "CHAPTER_ID = \"rnn\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "\n",
    "import errno\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "WORDS_PATH = \"datasets/words\"\n",
    "WORDS_URL = 'http://mattmahoney.net/dc/text8.zip'\n",
    "\n",
    "def mkdir_p(path):\n",
    "    \"\"\"Create directories, ok if they already exist.\n",
    "    \n",
    "    This is for python 2 support. In python >=3.2, simply use:\n",
    "    >>> os.makedirs(path, exist_ok=True)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #  os.makedirs(path)\n",
    "        os.makedirs(path,exist_ok=True)\n",
    "    except OSError as exc:\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def fetch_words_data(words_url=WORDS_URL, words_path=WORDS_PATH):\n",
    "    os.makedirs(words_path, exist_ok=True)\n",
    "    zip_path = os.path.join(words_path, \"words.zip\")\n",
    "    if not os.path.exists(zip_path):\n",
    "        urllib.request.urlretrieve(words_url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path) as f:\n",
    "        data = f.read(f.namelist()[0])\n",
    "    return data.decode(\"ascii\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = fetch_words_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[70090:70100]\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "vocabulary_size = 50000\n",
    "\n",
    "vocabulary = [(\"UNK\", None)] + Counter(words).most_common(vocabulary_size - 1)\n",
    "vocabulary = np.array([word for word, _ in vocabulary])\n",
    "dictionary = {word: code for code, word in enumerate(vocabulary)}\n",
    "data = np.array([dictionary.get(word, 0) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50000,)\n",
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'candela': 37080,\n",
       " 'errant': 40562,\n",
       " 'purposefully': 45239,\n",
       " 'roster': 12433,\n",
       " 'smile': 13903,\n",
       " 'islas': 28471,\n",
       " 'conducts': 15505,\n",
       " 'chiefs': 5876,\n",
       " 'grit': 36075,\n",
       " 'cadets': 29210,\n",
       " 'enforced': 7152,\n",
       " 'microkernel': 20989,\n",
       " 'assemblers': 25923,\n",
       " 'reviled': 31191,\n",
       " 'evoked': 27958,\n",
       " 'belgian': 3895,\n",
       " 'puck': 17096,\n",
       " 'aphids': 40143,\n",
       " 'limburgish': 30553,\n",
       " 'kekkonen': 47137,\n",
       " 'inclinations': 26185,\n",
       " 'either': 344,\n",
       " 'barnet': 41479,\n",
       " 'barbarossa': 18223,\n",
       " 'schoolmaster': 36725,\n",
       " 'hexagon': 36076,\n",
       " 'stalactites': 40563,\n",
       " 'jennifer': 10655,\n",
       " 'univ': 14023,\n",
       " 'grissom': 22154,\n",
       " 'crossroads': 12846,\n",
       " 'mercantilism': 13391,\n",
       " 'finder': 14718,\n",
       " 'journey': 3272,\n",
       " 'nephite': 49436,\n",
       " 'potion': 26793,\n",
       " 'sweet': 4412,\n",
       " 'haggard': 33309,\n",
       " 'clodius': 23331,\n",
       " 'ebert': 18742,\n",
       " 'larvae': 16821,\n",
       " 'voluminous': 27205,\n",
       " 'astana': 38187,\n",
       " 'buffet': 27959,\n",
       " 'risking': 30107,\n",
       " 'istria': 27054,\n",
       " 'remonstrance': 38188,\n",
       " 'pus': 39701,\n",
       " 'fermionic': 44097,\n",
       " 'subfamily': 13020,\n",
       " 'apologised': 35784,\n",
       " 'huron': 13516,\n",
       " 'lemy': 41480,\n",
       " 'isaacson': 47916,\n",
       " 'atkins': 12185,\n",
       " 'recension': 43524,\n",
       " 'habit': 7655,\n",
       " 'cameroun': 45868,\n",
       " 'derelict': 42951,\n",
       " 'pits': 14260,\n",
       " 'realize': 7987,\n",
       " 'intellect': 12353,\n",
       " 'commodities': 5676,\n",
       " 'gaming': 6384,\n",
       " 'upto': 36077,\n",
       " 'overuse': 32122,\n",
       " 'agents': 2881,\n",
       " 'armchair': 37431,\n",
       " 'rich': 1611,\n",
       " 'hart': 6843,\n",
       " 'transportation': 1883,\n",
       " 'bolshevik': 8460,\n",
       " 'libertarians': 16029,\n",
       " 'exceeding': 10949,\n",
       " 'orchestration': 26753,\n",
       " 'novelization': 41027,\n",
       " 'kant': 5223,\n",
       " 'irreverent': 39702,\n",
       " 'malvales': 38190,\n",
       " 'consecutive': 4816,\n",
       " 'halicarnassus': 38932,\n",
       " 'imbalance': 19353,\n",
       " 'embellished': 38191,\n",
       " 'imperative': 9620,\n",
       " 'andros': 39704,\n",
       " 'imperatoribus': 38935,\n",
       " 'struct': 35229,\n",
       " 'gifted': 12693,\n",
       " 'conclude': 8375,\n",
       " 'nicene': 8699,\n",
       " 'wrappings': 36391,\n",
       " 'amicable': 24959,\n",
       " 'velar': 14767,\n",
       " 'hipper': 25358,\n",
       " 'liberia': 6003,\n",
       " 'cozy': 39703,\n",
       " 'negligible': 10811,\n",
       " 'harmonic': 5057,\n",
       " 'undeclared': 45803,\n",
       " 'scooby': 35632,\n",
       " 'autistics': 32123,\n",
       " 'shortage': 11774,\n",
       " 'duty': 3353,\n",
       " 'bight': 26895,\n",
       " 'teng': 37859,\n",
       " 'scripts': 6932,\n",
       " 'marketshare': 46525,\n",
       " 'hudsucker': 48498,\n",
       " 'helsinki': 7501,\n",
       " 'feeble': 26754,\n",
       " 'corvette': 31861,\n",
       " 'unclaimed': 41318,\n",
       " 'juliana': 27350,\n",
       " 'pace': 6351,\n",
       " 'escape': 2672,\n",
       " 'unacceptably': 42952,\n",
       " 'horticulture': 28821,\n",
       " 'corrupts': 42433,\n",
       " 'teutoburg': 45242,\n",
       " 'rein': 33077,\n",
       " 'gnu': 2837,\n",
       " 'star': 592,\n",
       " 'mommy': 43525,\n",
       " 'gratis': 35785,\n",
       " 'dios': 43526,\n",
       " 'cuon': 32354,\n",
       " 'squarepants': 42950,\n",
       " 'destiny': 8850,\n",
       " 'stratofortress': 35786,\n",
       " 'lymphocytes': 35502,\n",
       " 'papa': 17878,\n",
       " 'earhart': 29982,\n",
       " 'grey': 3678,\n",
       " 'claire': 14623,\n",
       " 'confrontation': 11050,\n",
       " 'heresy': 6248,\n",
       " 'lowell': 13990,\n",
       " 'teach': 4075,\n",
       " 'monsignor': 47526,\n",
       " 'oil': 895,\n",
       " 'culbert': 42435,\n",
       " 'arcology': 35230,\n",
       " 'insurance': 3425,\n",
       " 'neri': 41988,\n",
       " 'ship': 1159,\n",
       " 'surfing': 16542,\n",
       " 'divorce': 5263,\n",
       " 'academy': 1410,\n",
       " 'rests': 8762,\n",
       " 'marshy': 28822,\n",
       " 'invoke': 15201,\n",
       " 'achaemenid': 16186,\n",
       " 'landgrave': 41989,\n",
       " 'daniel': 2274,\n",
       " 'epistemic': 28175,\n",
       " 'abode': 14995,\n",
       " 'babes': 32360,\n",
       " 'daughters': 5037,\n",
       " 'impairment': 12974,\n",
       " 'aeacus': 28388,\n",
       " 'ely': 24728,\n",
       " 'allegro': 23230,\n",
       " 'grandeur': 23085,\n",
       " 'refuse': 7928,\n",
       " 'handkerchief': 49374,\n",
       " 'pausanias': 19957,\n",
       " 'herg': 17742,\n",
       " 'brabham': 26186,\n",
       " 'tails': 12551,\n",
       " 'clunies': 47135,\n",
       " 'shipping': 5309,\n",
       " 'coded': 8879,\n",
       " 'kabul': 6226,\n",
       " 'powerpc': 9442,\n",
       " 'immunities': 38936,\n",
       " 'dilated': 37797,\n",
       " 'eiffel': 6879,\n",
       " 'cdna': 47918,\n",
       " 'bagpipes': 16770,\n",
       " 'snell': 34667,\n",
       " 'sobriquet': 40144,\n",
       " 'farrell': 16822,\n",
       " 'wend': 42954,\n",
       " 'salman': 30108,\n",
       " 'grows': 7455,\n",
       " 'heston': 41990,\n",
       " 'hern': 20691,\n",
       " 'consort': 11026,\n",
       " 'delusion': 24431,\n",
       " 'hover': 27352,\n",
       " 'theirs': 17195,\n",
       " 'idealists': 40564,\n",
       " 'fisheries': 10539,\n",
       " 'rameses': 48706,\n",
       " 'nikolaus': 26187,\n",
       " 'azores': 17456,\n",
       " 'sufficed': 48623,\n",
       " 'accorded': 14437,\n",
       " 'ji': 12034,\n",
       " 'genes': 3476,\n",
       " 'basso': 20566,\n",
       " 'bronfman': 47919,\n",
       " 'attainder': 45243,\n",
       " 'rationalism': 13259,\n",
       " 'northern': 475,\n",
       " 'clones': 9473,\n",
       " 'displacing': 22155,\n",
       " 'lorenz': 10180,\n",
       " 'describing': 3337,\n",
       " 'macromolecule': 41837,\n",
       " 'factbook': 4740,\n",
       " 'warships': 12138,\n",
       " 'provability': 49375,\n",
       " 'brit': 18436,\n",
       " 'sociology': 7623,\n",
       " 'dizziness': 28124,\n",
       " 'separable': 22609,\n",
       " 'waring': 42955,\n",
       " 'mast': 15528,\n",
       " 'protector': 12225,\n",
       " 'contrapuntal': 20409,\n",
       " 'cub': 24853,\n",
       " 'garret': 29731,\n",
       " 'dramas': 15426,\n",
       " 'tilted': 26471,\n",
       " 'laboratory': 2975,\n",
       " 'ofcom': 44100,\n",
       " 'patents': 5836,\n",
       " 'ssang': 47598,\n",
       " 'sick': 6848,\n",
       " 'geocache': 48393,\n",
       " 'etext': 29918,\n",
       " 'simulcast': 46807,\n",
       " 'bluntly': 49372,\n",
       " 'mallow': 32363,\n",
       " 'mindedness': 42437,\n",
       " 'iraqis': 30554,\n",
       " 'brilliant': 7041,\n",
       " 'smoky': 27506,\n",
       " 'airplanes': 12496,\n",
       " 'called': 89,\n",
       " 'nurse': 10158,\n",
       " 'infantile': 31662,\n",
       " 'disarm': 34948,\n",
       " 'employed': 2395,\n",
       " 'vivaldi': 14886,\n",
       " 'wingers': 28280,\n",
       " 'kaiserliche': 42438,\n",
       " 'comprise': 6663,\n",
       " 'dbv': 45240,\n",
       " 'sequel': 5566,\n",
       " 'confronts': 30965,\n",
       " 'eiji': 43453,\n",
       " 'andrei': 13675,\n",
       " 'questioning': 12160,\n",
       " 'lovingly': 43539,\n",
       " 'protective': 6910,\n",
       " 'pastimes': 43344,\n",
       " 'obliterated': 40565,\n",
       " 'zy': 48624,\n",
       " 'haemoglobin': 44101,\n",
       " 'dashing': 43798,\n",
       " 'planetfall': 42957,\n",
       " 'moorish': 15068,\n",
       " 'lawrencium': 36393,\n",
       " 'threat': 2669,\n",
       " 'threats': 6274,\n",
       " 'revulsion': 47892,\n",
       " 'bubble': 6362,\n",
       " 'panamanian': 36394,\n",
       " 'erebus': 34668,\n",
       " 'eject': 27206,\n",
       " 'namek': 36726,\n",
       " 'witherspoon': 47397,\n",
       " 'romney': 33602,\n",
       " 'pitching': 9157,\n",
       " 'raise': 3866,\n",
       " 'normalcy': 45869,\n",
       " 'pva': 42958,\n",
       " 'conway': 12774,\n",
       " 'ibf': 46811,\n",
       " 'waylon': 28632,\n",
       " 'flocks': 25776,\n",
       " 'agatha': 16823,\n",
       " 'pseudorandom': 41482,\n",
       " 'affecting': 7469,\n",
       " 'lagenorhynchus': 39333,\n",
       " 'enid': 27800,\n",
       " 'milo': 11517,\n",
       " 'eskimos': 32355,\n",
       " 'talmadge': 44614,\n",
       " 'slogan': 8094,\n",
       " 'cryopreserved': 41182,\n",
       " 'sacramento': 13451,\n",
       " 'expect': 6131,\n",
       " 'ocaml': 44615,\n",
       " 'metaxas': 49753,\n",
       " 'insensitivity': 41992,\n",
       " 'diena': 31663,\n",
       " 'drain': 10524,\n",
       " 'cursus': 30565,\n",
       " 'gting': 42959,\n",
       " 'obtains': 16074,\n",
       " 'afb': 12817,\n",
       " 'begotten': 19958,\n",
       " 'shown': 1093,\n",
       " 'erick': 44103,\n",
       " 'electrodes': 13118,\n",
       " 'franchise': 3727,\n",
       " 'storehouse': 33318,\n",
       " 'annapolis': 19167,\n",
       " 'switzer': 43546,\n",
       " 'managerial': 25229,\n",
       " 'carla': 23221,\n",
       " 'keegan': 30234,\n",
       " 'shares': 4087,\n",
       " 'reprinted': 6706,\n",
       " 'gascon': 31322,\n",
       " 'dylan': 3115,\n",
       " 'cloudy': 22515,\n",
       " 'governance': 8140,\n",
       " 'apteryx': 45870,\n",
       " 'offshoot': 16109,\n",
       " 'timberlake': 49376,\n",
       " 'baresi': 40566,\n",
       " 'infamy': 38573,\n",
       " 'chuppah': 37433,\n",
       " 'detector': 13414,\n",
       " 'submits': 31278,\n",
       " 'tunable': 48626,\n",
       " 'bodhisattva': 15427,\n",
       " 'offending': 21467,\n",
       " 'oic': 18224,\n",
       " 'merits': 10950,\n",
       " 'censorship': 5722,\n",
       " 'burgundian': 19081,\n",
       " 'barbiturates': 28281,\n",
       " 'initiative': 5281,\n",
       " 'grinch': 30972,\n",
       " 'predominated': 35365,\n",
       " 'workmen': 31440,\n",
       " 'quests': 34108,\n",
       " 'gheorghe': 47139,\n",
       " 'debussy': 13487,\n",
       " 'telephones': 5827,\n",
       " 'waist': 15840,\n",
       " 'martyred': 18914,\n",
       " 'imagine': 8472,\n",
       " 'nikolaevich': 35787,\n",
       " 'paranoia': 19496,\n",
       " 'goddesses': 8175,\n",
       " 'marquis': 10271,\n",
       " 'boeing': 3488,\n",
       " 'caicos': 33304,\n",
       " 'transform': 3860,\n",
       " 'salutation': 42440,\n",
       " 'ediacaran': 45871,\n",
       " 'anarchistic': 44616,\n",
       " 'tsang': 28282,\n",
       " 'pistols': 12017,\n",
       " 'denying': 10369,\n",
       " 'chaotic': 9705,\n",
       " 'marquess': 14217,\n",
       " 'surrounds': 15878,\n",
       " 'taliaferro': 38189,\n",
       " 'property': 918,\n",
       " 'wftu': 18683,\n",
       " 'havens': 32396,\n",
       " 'comparable': 5127,\n",
       " 'hunchback': 27839,\n",
       " 'uranus': 15097,\n",
       " 'badges': 24857,\n",
       " 'complication': 25098,\n",
       " 'paleontologist': 26189,\n",
       " 'tactical': 6473,\n",
       " 'bally': 45872,\n",
       " 'communaut': 32815,\n",
       " 'defective': 14965,\n",
       " 'singing': 3669,\n",
       " 'discount': 16225,\n",
       " 'lace': 22156,\n",
       " 'bertie': 30761,\n",
       " 'complicated': 3813,\n",
       " 'lakeside': 42441,\n",
       " 'chalmers': 19168,\n",
       " 'sidney': 11150,\n",
       " 'canon': 3323,\n",
       " 'traversing': 33321,\n",
       " 'rah': 36728,\n",
       " 'magen': 34388,\n",
       " 'dilapidated': 40147,\n",
       " 'digby': 36395,\n",
       " 'cheat': 19410,\n",
       " 'fortuna': 42442,\n",
       " 'evacuated': 11992,\n",
       " 'accordingly': 5712,\n",
       " 'valid': 3466,\n",
       " 'questionnaire': 28823,\n",
       " 'camelopardalis': 29373,\n",
       " 'dialects': 1990,\n",
       " 'tie': 5563,\n",
       " 'roam': 26050,\n",
       " 'fertilizers': 22801,\n",
       " 'preeminent': 30109,\n",
       " 'strangelove': 17009,\n",
       " 'augmenting': 43527,\n",
       " 'perez': 21150,\n",
       " 'fifo': 28633,\n",
       " 'coverage': 4034,\n",
       " 'formless': 36729,\n",
       " 'herbaceous': 27056,\n",
       " 'salted': 27801,\n",
       " 'pestilence': 31192,\n",
       " 'kafka': 10196,\n",
       " 'cluster': 4708,\n",
       " 'nih': 16634,\n",
       " 'enters': 5991,\n",
       " 'manna': 33552,\n",
       " 'curds': 36078,\n",
       " 'judo': 9918,\n",
       " 'thoreau': 28284,\n",
       " 'revival': 4229,\n",
       " 'abbeville': 41993,\n",
       " 'atsc': 33553,\n",
       " 'wormwood': 19301,\n",
       " 'ranging': 4279,\n",
       " 'parsimony': 38381,\n",
       " 'towers': 6754,\n",
       " 'burdens': 26051,\n",
       " 'spared': 15429,\n",
       " 'shouldn': 19568,\n",
       " 'slocum': 39336,\n",
       " 'awareness': 5441,\n",
       " 'bulging': 41994,\n",
       " 'waging': 23086,\n",
       " 'compared': 1428,\n",
       " 'forestall': 32356,\n",
       " 'honky': 32357,\n",
       " 'nag': 18498,\n",
       " 'loonie': 30966,\n",
       " 'develops': 8056,\n",
       " 'cumulant': 47140,\n",
       " 'clairvaux': 23549,\n",
       " 'calculated': 4895,\n",
       " 'desk': 8346,\n",
       " 'stowe': 31664,\n",
       " 'hercules': 8400,\n",
       " 'israelites': 6924,\n",
       " 'icbms': 22978,\n",
       " 'katanga': 25360,\n",
       " 'fonni': 47921,\n",
       " 'halogen': 25361,\n",
       " 'adaption': 34951,\n",
       " 'hershey': 28286,\n",
       " 'prof': 15679,\n",
       " 'bissau': 9893,\n",
       " 'bickering': 49377,\n",
       " 'willamette': 26869,\n",
       " 'bon': 17302,\n",
       " 'disturbs': 47923,\n",
       " 'peasants': 6182,\n",
       " 'crafted': 16267,\n",
       " 'staffed': 21396,\n",
       " 'azul': 42960,\n",
       " 'writing': 716,\n",
       " 'concepts': 1744,\n",
       " 'nad': 20304,\n",
       " 'th': 91,\n",
       " 'evidenced': 12055,\n",
       " 'declines': 16410,\n",
       " 'authors': 2013,\n",
       " 'natively': 19959,\n",
       " 'double': 885,\n",
       " 'encyclopedist': 47924,\n",
       " 'andersonville': 28473,\n",
       " 'arrogant': 17296,\n",
       " 'sunt': 36706,\n",
       " 'booted': 32816,\n",
       " 'errol': 27652,\n",
       " 'databases': 7905,\n",
       " 'preservative': 36079,\n",
       " 'view': 453,\n",
       " 'worlds': 4015,\n",
       " 'ragnar': 17446,\n",
       " 'triglycerides': 29374,\n",
       " 'manner': 1955,\n",
       " 'mail': 2216,\n",
       " 'pronunciation': 3066,\n",
       " 'equinoctial': 40148,\n",
       " 'underlies': 22610,\n",
       " 'schafer': 41028,\n",
       " 'haile': 16720,\n",
       " 'cauchy': 7362,\n",
       " 'portraiture': 48629,\n",
       " 'mojave': 36954,\n",
       " 'replete': 31432,\n",
       " 'socioeconomic': 22516,\n",
       " 'oysters': 30566,\n",
       " 'legacy': 2909,\n",
       " 'phipps': 31894,\n",
       " 'clearest': 32817,\n",
       " 'ammonia': 6471,\n",
       " 'benin': 11441,\n",
       " 'moore': 2448,\n",
       " 'ltcm': 39057,\n",
       " 'flashlight': 38434,\n",
       " 'electoral': 3263,\n",
       " 'vernet': 44622,\n",
       " 'sunsets': 39723,\n",
       " 'equinox': 9183,\n",
       " 'received': 639,\n",
       " 'daemen': 48158,\n",
       " 'foods': 4426,\n",
       " 'appalled': 31895,\n",
       " 'storylines': 14307,\n",
       " 'resentful': 36936,\n",
       " 'karlheinz': 30762,\n",
       " 'oau': 17880,\n",
       " 'isis': 15725,\n",
       " 'compatibles': 24729,\n",
       " 'faux': 22562,\n",
       " 'javan': 44617,\n",
       " 'anchor': 6510,\n",
       " 'befitting': 46527,\n",
       " 'commonest': 42961,\n",
       " 'wide': 796,\n",
       " 'flocked': 24983,\n",
       " 'television': 483,\n",
       " 'lfar': 32581,\n",
       " 'ingestion': 15879,\n",
       " 'vara': 44104,\n",
       " 'ewes': 48716,\n",
       " 'invasion': 1626,\n",
       " 'oswego': 41484,\n",
       " 'roadside': 36080,\n",
       " 'correspondences': 27057,\n",
       " 'griffon': 31665,\n",
       " 'inasmuch': 28287,\n",
       " 'romanesque': 23222,\n",
       " 'minimising': 41995,\n",
       " 'enumerative': 45874,\n",
       " 'ortho': 42962,\n",
       " 'stanshall': 39705,\n",
       " 'rehearsing': 47143,\n",
       " 'chanson': 40149,\n",
       " 'holomorphic': 15726,\n",
       " 'captives': 22073,\n",
       " 'ambient': 9336,\n",
       " 'duomo': 47144,\n",
       " 'drinking': 4028,\n",
       " 'decins': 39337,\n",
       " 'yak': 45246,\n",
       " 'aging': 8214,\n",
       " 'ferris': 37434,\n",
       " 'overcome': 6352,\n",
       " 'compaq': 13142,\n",
       " 'turf': 22343,\n",
       " 'brontosaurus': 43709,\n",
       " 'extends': 4562,\n",
       " 'societal': 15563,\n",
       " 'mistreatment': 25924,\n",
       " 'offsets': 41030,\n",
       " 'glidrose': 39706,\n",
       " 'tell': 2722,\n",
       " 'counteracting': 46528,\n",
       " 'wad': 46529,\n",
       " 'condensation': 13726,\n",
       " 'cremated': 15680,\n",
       " 'synthesizing': 28635,\n",
       " 'responsibility': 3063,\n",
       " 'obvious': 3366,\n",
       " 'inward': 14778,\n",
       " 'totleben': 49379,\n",
       " 'mortimer': 19082,\n",
       " 'scratching': 22344,\n",
       " 'peripheral': 9060,\n",
       " 'biscuit': 24525,\n",
       " 'polydor': 48630,\n",
       " 'christchurch': 30555,\n",
       " 'distinguish': 3140,\n",
       " 'barium': 20164,\n",
       " 'concentric': 20188,\n",
       " 'ioc': 7484,\n",
       " 'absalom': 16503,\n",
       " 'debugger': 19497,\n",
       " 'alston': 29021,\n",
       " 'irc': 4932,\n",
       " 'liang': 22517,\n",
       " 'gigantic': 15506,\n",
       " 'fancher': 39707,\n",
       " 'abridged': 16591,\n",
       " 'hardcover': 10020,\n",
       " 'tectonics': 20751,\n",
       " 'trigger': 5206,\n",
       " 'rajonas': 19543,\n",
       " 'cortex': 10448,\n",
       " 'hosted': 4736,\n",
       " 'impacts': 9297,\n",
       " 'discontinuities': 27654,\n",
       " 'dysplasia': 47730,\n",
       " 'shenzhen': 32353,\n",
       " 'fitting': 11307,\n",
       " 'herse': 47926,\n",
       " 'hurt': 7485,\n",
       " 'income': 1429,\n",
       " 'genitalia': 34800,\n",
       " 'ryle': 31896,\n",
       " 'goods': 1792,\n",
       " 'ln': 10072,\n",
       " 'margarine': 44653,\n",
       " 'adjoint': 20158,\n",
       " 'thorium': 27440,\n",
       " 'unlicensed': 28297,\n",
       " 'arising': 8669,\n",
       " 'courtney': 21641,\n",
       " 'wearer': 21563,\n",
       " 'noxious': 34952,\n",
       " 'ideology': 4789,\n",
       " 'bomis': 23550,\n",
       " 'bixby': 24625,\n",
       " 'default': 5234,\n",
       " 'heiress': 20612,\n",
       " 'kauravas': 44105,\n",
       " 'hora': 36731,\n",
       " 'bagpipe': 20565,\n",
       " 'om': 13412,\n",
       " 'ngc': 13587,\n",
       " 'forums': 6098,\n",
       " 'antoninianus': 45757,\n",
       " 'lysosomes': 31194,\n",
       " 'shortages': 12694,\n",
       " 'failing': 6502,\n",
       " 'choctaws': 26757,\n",
       " 'answers': 5469,\n",
       " 'nicomachean': 46032,\n",
       " 'manifest': 10637,\n",
       " 'lobes': 25055,\n",
       " 'mmx': 22611,\n",
       " 'olivier': 16771,\n",
       " 'olam': 28474,\n",
       " 'vw': 29727,\n",
       " 'fuji': 32358,\n",
       " 'oc': 17248,\n",
       " 'manifests': 21319,\n",
       " 'mos': 9116,\n",
       " 'forelimbs': 47927,\n",
       " 'homosexuals': 10032,\n",
       " 'dwan': 47145,\n",
       " 'leaked': 16643,\n",
       " 'hourglass': 39391,\n",
       " 'cort': 11027,\n",
       " 'mori': 31449,\n",
       " 'fez': 28824,\n",
       " 'knut': 30556,\n",
       " 'reinstatement': 41513,\n",
       " 'insane': 9810,\n",
       " 'lovers': 7752,\n",
       " 'quenched': 45556,\n",
       " 'attention': 1554,\n",
       " 'interregnum': 26480,\n",
       " 'graphing': 38965,\n",
       " 'breaches': 24063,\n",
       " 'hominidae': 44106,\n",
       " 'peng': 27960,\n",
       " 'reims': 38201,\n",
       " 'grieg': 21235,\n",
       " 'milestone': 13119,\n",
       " 'forefathers': 37082,\n",
       " 'dc': 2539,\n",
       " 'pie': 10933,\n",
       " 'leveled': 22979,\n",
       " 'planet': 1751,\n",
       " 'romeo': 19433,\n",
       " 'spent': 1764,\n",
       " 'ports': 3164,\n",
       " 'vibrations': 14847,\n",
       " 'cassandra': 16972,\n",
       " 'saddam': 8783,\n",
       " 'algonquin': 30142,\n",
       " 'eridanus': 45875,\n",
       " 'leanings': 22040,\n",
       " 'colonna': 24984,\n",
       " 'tsuburaya': 42588,\n",
       " 'companions': 9034,\n",
       " 'folklore': 5348,\n",
       " 'stinging': 29919,\n",
       " 'benzoic': 19880,\n",
       " 'denominational': 23422,\n",
       " 'blitzkrieg': 13361,\n",
       " 'idf': 16778,\n",
       " 'hoplite': 37800,\n",
       " 'girl': 2589,\n",
       " 'cpc': 9589,\n",
       " 'cleared': 9906,\n",
       " 'misfits': 28657,\n",
       " 'optimized': 16973,\n",
       " 'aardwolf': 43246,\n",
       " 'khartoum': 19024,\n",
       " 'metamorphic': 21257,\n",
       " 'brabant': 11518,\n",
       " 'dcc': 35231,\n",
       " 'aerodrome': 43529,\n",
       " 'disturbances': 15098,\n",
       " 'mta': 28389,\n",
       " 'wrecks': 35503,\n",
       " 'adventurer': 18036,\n",
       " 'trevelyan': 47928,\n",
       " 'curdled': 40238,\n",
       " 'falsehood': 21151,\n",
       " 'vivekananda': 45929,\n",
       " 'laine': 36396,\n",
       " 'seifert': 39339,\n",
       " 'bowell': 34953,\n",
       " 'ziibi': 47146,\n",
       " 'vander': 43575,\n",
       " 'initiates': 18548,\n",
       " 'monarchical': 20089,\n",
       " 'realities': 13565,\n",
       " 'tiraspol': 47147,\n",
       " 'invaluable': 23647,\n",
       " 'hesychast': 20033,\n",
       " 'hailie': 32820,\n",
       " 'bromley': 37436,\n",
       " 'alomar': 47148,\n",
       " 'undergo': 6984,\n",
       " 'bouldering': 20690,\n",
       " 'dorpat': 44619,\n",
       " 'clausewitz': 18793,\n",
       " 'monoid': 13440,\n",
       " 'obedience': 9488,\n",
       " 'efforts': 1447,\n",
       " 'nucleation': 46530,\n",
       " 'nicea': 28125,\n",
       " 'threading': 33804,\n",
       " 'deutsche': 11410,\n",
       " 'pakistanis': 38575,\n",
       " 'skyscraper': 16359,\n",
       " 'audi': 7712,\n",
       " 'midwives': 15132,\n",
       " 'sergius': 34670,\n",
       " 'richie': 22249,\n",
       " 'dorothy': 8041,\n",
       " 'seats': 1922,\n",
       " 'three': 17,\n",
       " 'wei': 18621,\n",
       " 'perdition': 46531,\n",
       " 'sitting': 5240,\n",
       " 'awarded': 1924,\n",
       " 'skates': 26472,\n",
       " 'kerala': 7730,\n",
       " 'qui': 12552,\n",
       " 'vcr': 28475,\n",
       " 'manifesting': 35504,\n",
       " 'ha': 4922,\n",
       " 'invoking': 21729,\n",
       " 'wichita': 21794,\n",
       " 'buda': 39708,\n",
       " 'charted': 20519,\n",
       " 'monophosphate': 37464,\n",
       " 'booker': 21320,\n",
       " 'midrash': 11545,\n",
       " 'vojvodina': 32359,\n",
       " 'patrician': 20990,\n",
       " 'cutbacks': 34389,\n",
       " 'anniversary': 4459,\n",
       " 'jarrett': 42443,\n",
       " 'terri': 34672,\n",
       " 'descartes': 8890,\n",
       " 'plato': 5230,\n",
       " 'sanctify': 44620,\n",
       " 'avengers': 17881,\n",
       " 'svetlana': 31195,\n",
       " 'langue': 24526,\n",
       " 'punta': 42733,\n",
       " 'computability': 21152,\n",
       " 'pedagogue': 33082,\n",
       " 'enlist': 21887,\n",
       " 'manometer': 42964,\n",
       " 'escorted': 33313,\n",
       " 'fungus': 16772,\n",
       " 'tong': 28239,\n",
       " 'shalmaneser': 25100,\n",
       " 'dowry': 24433,\n",
       " 'macht': 36102,\n",
       " 'finch': 20520,\n",
       " 'middleweight': 31898,\n",
       " 'heidegger': 13517,\n",
       " 'patten': 47929,\n",
       " 'belgrano': 33556,\n",
       " 'marguerite': 19944,\n",
       " 'cushitic': 19025,\n",
       " 'intra': 18601,\n",
       " 'jes': 37919,\n",
       " 'maloney': 42714,\n",
       " 'closet': 21469,\n",
       " 'dems': 26473,\n",
       " 'supplier': 13383,\n",
       " 'redrawn': 41524,\n",
       " 'dominican': 4885,\n",
       " 'mimics': 30763,\n",
       " 'hyksos': 33685,\n",
       " 'dagda': 37083,\n",
       " 'starter': 16154,\n",
       " 'taxpayer': 28126,\n",
       " 'illustration': 6830,\n",
       " 'adults': 4522,\n",
       " 'conventional': 2545,\n",
       " 'expressway': 12373,\n",
       " 'monophonic': 38703,\n",
       " 'nigerian': 14646,\n",
       " 'olsen': 18915,\n",
       " 'bmw': 5254,\n",
       " 'lycaon': 25925,\n",
       " 'chisel': 21470,\n",
       " 'bessel': 11695,\n",
       " 'barbarians': 14050,\n",
       " 'beryllium': 10902,\n",
       " 'aorist': 35505,\n",
       " 'steffi': 41031,\n",
       " 'limericks': 43247,\n",
       " 'stretching': 9035,\n",
       " 'favourite': 7374,\n",
       " 'elegance': 24854,\n",
       " 'recommendation': 9167,\n",
       " 'enlargement': 18099,\n",
       " 'cracow': 37929,\n",
       " 'affords': 42444,\n",
       " 'pedagogy': 25492,\n",
       " 'creatively': 39710,\n",
       " 'drayton': 43530,\n",
       " 'immortality': 10656,\n",
       " 'harbors': 9443,\n",
       " 'householder': 14375,\n",
       " 'amicus': 42217,\n",
       " 'southwest': 3230,\n",
       " 'vail': 35506,\n",
       " 'curule': 34138,\n",
       " 'owner': 3060,\n",
       " 'marx': 2985,\n",
       " 'christoph': 18499,\n",
       " 'dagestan': 35507,\n",
       " 'halftime': 34238,\n",
       " 'steyn': 41032,\n",
       " 'uucp': 28825,\n",
       " 'maria': 2910,\n",
       " 'mst': 17010,\n",
       " 'linebacker': 14779,\n",
       " 'movers': 38194,\n",
       " 'fundamental': 1750,\n",
       " 'underlying': 4134,\n",
       " 'exegetical': 34673,\n",
       " 'undefeated': 20913,\n",
       " 'chance': 2518,\n",
       " 'subdominant': 37438,\n",
       " 'adic': 25230,\n",
       " 'sabah': 13143,\n",
       " 'gypsy': 13695,\n",
       " 'zambezi': 28127,\n",
       " 'ditches': 36083,\n",
       " 'improvisational': 18862,\n",
       " 'encamped': 34442,\n",
       " 'ago': 2377,\n",
       " 'electronvolt': 49381,\n",
       " 'interesting': 2650,\n",
       " 'yb': 28636,\n",
       " 'matre': 39748,\n",
       " 'upright': 9265,\n",
       " 'popov': 42965,\n",
       " 'digital': 926,\n",
       " 'bedding': 41033,\n",
       " 'extraordinarily': 14996,\n",
       " 'airing': 17930,\n",
       " 'spraying': 24626,\n",
       " 'meson': 25369,\n",
       " 'rick': 7207,\n",
       " 'cuauht': 26653,\n",
       " 'unprepared': 25056,\n",
       " 'cues': 24173,\n",
       " 'five': 16,\n",
       " 'pleases': 49382,\n",
       " 'gesserit': 11083,\n",
       " 'defenseless': 40568,\n",
       " 'horner': 27188,\n",
       " 'lassa': 29031,\n",
       " 'cerium': 14087,\n",
       " 'kword': 41487,\n",
       " 'kabbalist': 43591,\n",
       " 'atypical': 17413,\n",
       " 'kanagawa': 45558,\n",
       " 'yard': 6768,\n",
       " 'old': 246,\n",
       " 'praetor': 24473,\n",
       " 'sultanates': 47984,\n",
       " 'fallacious': 25926,\n",
       " 'prodigy': 16671,\n",
       " 'paratroopers': 24730,\n",
       " 'orientational': 31455,\n",
       " 'picture': 1741,\n",
       " 'borderline': 27060,\n",
       " 'multilingual': 18037,\n",
       " 'roost': 48632,\n",
       " 'extensible': 22250,\n",
       " 'customs': 3848,\n",
       " 'usr': 42966,\n",
       " 'locality': 14574,\n",
       " 'tunisia': 9346,\n",
       " 'aloe': 8764,\n",
       " 'chao': 28637,\n",
       " 'quinn': 14887,\n",
       " 'percy': 9905,\n",
       " 'agates': 46613,\n",
       " 'computationally': 26442,\n",
       " 'millennialism': 30558,\n",
       " 'remixing': 46937,\n",
       " 'biblioth': 40152,\n",
       " 'praetorian': 18917,\n",
       " 'luanda': 20707,\n",
       " 'suppose': 5564,\n",
       " 'kt': 22157,\n",
       " 'counseling': 19881,\n",
       " 'transmembrane': 30786,\n",
       " 'stanislavsky': 32583,\n",
       " 'reuben': 17688,\n",
       " 'nursia': 41488,\n",
       " 'proxy': 12647,\n",
       " 'teller': 17356,\n",
       " 'playmate': 49731,\n",
       " 'move': 984,\n",
       " 'rubbish': 31899,\n",
       " 'sirhan': 46532,\n",
       " 'sunset': 8750,\n",
       " 'ales': 19882,\n",
       " 'pernambuco': 32584,\n",
       " 'aulus': 34954,\n",
       " 'cycorp': 47931,\n",
       " 'holden': 11427,\n",
       " 'precedence': 14671,\n",
       " 'tertiary': 7547,\n",
       " 'warranted': 27655,\n",
       " 'bituminous': 41481,\n",
       " 'norma': 17987,\n",
       " 'countermeasure': 40153,\n",
       " 'musculus': 37801,\n",
       " 'calculate': 6626,\n",
       " 'animations': 16316,\n",
       " 'prevalence': 9337,\n",
       " 'gos': 37802,\n",
       " 'pedro': 7798,\n",
       " 'clinician': 42967,\n",
       " 'congestion': 16721,\n",
       " 'imitated': 14997,\n",
       " 'chanted': 32387,\n",
       " 'robbing': 42953,\n",
       " 'posthumus': 41429,\n",
       " 'vy': 34110,\n",
       " 'csce': 49383,\n",
       " 'evers': 19883,\n",
       " 'transduction': 20773,\n",
       " 'shanahan': 42968,\n",
       " 'steinsaltz': 28826,\n",
       " 'congressional': 6813,\n",
       " 'aloft': 41489,\n",
       " 'holt': 12035,\n",
       " 'oases': 23737,\n",
       " 'ici': 28288,\n",
       " 'rebirth': 14584,\n",
       " 'llewellyn': 39711,\n",
       " 'persuasion': 17297,\n",
       " 'pudding': 18500,\n",
       " 'booths': 26613,\n",
       " 'documentation': 4964,\n",
       " 'mills': 8008,\n",
       " 'cervix': 29556,\n",
       " 'bri': 47151,\n",
       " 'ldp': 19812,\n",
       " 'stenella': 41034,\n",
       " 'gion': 20311,\n",
       " 'ruminants': 38576,\n",
       " 'intersex': 30344,\n",
       " 'squads': 21888,\n",
       " 'rhetoric': 7891,\n",
       " 'nou': 43532,\n",
       " 'kilt': 13473,\n",
       " 'knocked': 14888,\n",
       " 'hatfield': 12434,\n",
       " 'hexokinase': 47933,\n",
       " 'complained': 10680,\n",
       " 'gallia': 25157,\n",
       " 'maximal': 17533,\n",
       " 'ttt': 43892,\n",
       " 'suprema': 47152,\n",
       " 'threw': 8194,\n",
       " 'extend': 3876,\n",
       " 'formality': 19498,\n",
       " 'hutchence': 29431,\n",
       " 'whistles': 33084,\n",
       " 'stronghold': 12596,\n",
       " 'tenor': 11015,\n",
       " 'oss': 24271,\n",
       " 'forrester': 26324,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(vocabulary))\n",
    "print(vocabulary.shape)\n",
    "vocabulary\n",
    "print(type(dictionary))\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "903\n",
      "243\n",
      "1012\n",
      "1908\n",
      "10158\n"
     ]
    }
   ],
   "source": [
    "print(dictionary['king'])\n",
    "print(dictionary['queen'])\n",
    "print(dictionary['man'])\n",
    "print(dictionary['woman'])\n",
    "print(dictionary['doctor'])\n",
    "print(dictionary['nurse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('anarchism originated as a term of abuse first used',\n",
       " array([5242, 3084,   12,    6,  195,    2, 3136,   46,   59]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(words[:9]), data[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'belonging matrix as a term of presidency first used'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([vocabulary[word_index] for word_index in [5241, 3081, 12, 6, 195, 2, 3134, 46, 59]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('that', 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[255], data[255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index=0\n",
    "batch, labels = generate_batch(8, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3084, 3084,   12,   12,    6,    6,  195,  195]),\n",
       " ['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, [vocabulary[word] for word in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  12],\n",
       "        [5242],\n",
       "        [   6],\n",
       "        [3084],\n",
       "        [  12],\n",
       "        [ 195],\n",
       "        [   2],\n",
       "        [   6]]),\n",
       " ['as', 'anarchism', 'a', 'originated', 'as', 'term', 'of', 'a'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, [vocabulary[word] for word in labels[:, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# Input data.\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "embedding_size = 150\n",
    "\n",
    "# Look up embeddings for inputs.\n",
    "init_embeds = tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "embeddings = tf.Variable(init_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the variables for the NCE loss\n",
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                        stddev=1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Compute the average NCE loss for the batch.\n",
    "# tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "# time we evaluate the loss.\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(nce_weights, nce_biases, train_labels, embed,\n",
    "                   num_sampled, vocabulary_size))\n",
    "\n",
    "# Construct the Adam optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), axis=1, keepdims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "# Add variable initializer.\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\tAverage loss at step  0 :  298.5007019042969\n",
      "Nearest to many: personalized, electronegative, prized, experiencing, stringing, dhtml, jaffna, cmd,\n",
      "Nearest to from: vehicle, fleurs, continuance, curt, matthias, thermally, positrons, pir,\n",
      "Nearest to there: til, foix, anabaptist, seems, firewood, reactionary, expertise, operational,\n",
      "Nearest to all: accountable, undertones, popes, documentaries, adsl, shehri, homilies, nurture,\n",
      "Nearest to they: wettest, strasbourg, pseudocode, havoc, discredited, ammonite, cellist, andover,\n",
      "Nearest to war: partitive, igbo, tiamat, freenode, overwrite, protege, pierluigi, fulfilling,\n",
      "Nearest to four: valencia, markings, personnel, qing, kekul, blew, stellated, pillaging,\n",
      "Nearest to history: transistor, wct, seifert, authorize, urbana, rearranging, racecar, clarinet,\n",
      "Nearest to th: streetcars, compressors, crystal, quantized, arlington, assays, craving, philo,\n",
      "Nearest to one: ids, malthusian, multiplies, patinkin, trolleybus, alan, mongolian, inventive,\n",
      "Nearest to see: gomorrah, flavoured, manipulates, authorship, skeptic, situ, philosophiae, testimonium,\n",
      "Nearest to would: render, catan, savanna, tarragona, fucked, galton, carter, historiography,\n",
      "Nearest to three: bartleby, diners, yucat, substituting, labored, wads, minnie, andrei,\n",
      "Nearest to into: anjouan, soot, radians, mesoamerica, tabloids, reminds, bun, autonomously,\n",
      "Nearest to been: laurent, killian, computes, riches, jogging, ctus, chemotaxis, insignia,\n",
      "Nearest to has: disgrace, grew, recognising, preached, defaults, tius, flocks, usta,\n",
      "Iteration: 2000\tAverage loss at step  2000 :  132.0740223608017\n",
      "Iteration: 4000\tAverage loss at step  4000 :  62.37627522349358\n",
      "Iteration: 6000\tAverage loss at step  6000 :  41.728235058784485\n",
      "Iteration: 8000\tAverage loss at step  8000 :  31.141834504961967\n",
      "Iteration: 10000\tAverage loss at step  10000 :  25.66445340538025\n",
      "Nearest to many: alp, nato, ads, the, cushitic, altaic, milne, slain,\n",
      "Nearest to from: in, with, argon, fewer, categorization, of, seo, are,\n",
      "Nearest to there: also, tarkovsky, romanus, consumed, tezuka, which, lowry, making,\n",
      "Nearest to all: that, frau, festival, portraits, jonathan, golf, dealings, xserve,\n",
      "Nearest to they: sidebands, qxd, we, unexplained, users, ankh, benedictine, dictated,\n",
      "Nearest to war: kick, nine, solanas, graph, eight, cockpit, sod, brakes,\n",
      "Nearest to four: one, three, nine, two, five, eight, six, zero,\n",
      "Nearest to history: electrolysis, afghana, sworn, branden, antigua, milne, ampere, spoof,\n",
      "Nearest to th: darker, three, shut, one, steffi, eight, four, bombers,\n",
      "Nearest to one: three, nine, eight, four, five, two, six, seven,\n",
      "Nearest to see: abbots, bicycle, of, dealings, organs, wasn, anniversaries, amir,\n",
      "Nearest to would: abdicated, deuterostomes, should, refusal, ludovico, booming, to, secretly,\n",
      "Nearest to three: one, four, eight, two, six, nine, five, seven,\n",
      "Nearest to into: anodes, sixteenth, gide, kemp, spooner, to, psalms, ductile,\n",
      "Nearest to been: has, valens, have, reasonably, deutschland, aussie, already, avenue,\n",
      "Nearest to has: have, had, closures, getting, critics, is, in, serif,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        print(\"\\rIteration: {}\".format(step), end=\"\\t\")\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n",
    "\n",
    "        # We perform one update step by evaluating the training op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([training_op, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print(\"Average loss at step \", step, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = vocabulary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log_str = \"Nearest to %s:\" % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = vocabulary[nearest[k]]\n",
    "                    log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                print(log_str)\n",
    "\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. We find that the final_embeddings produced by the code above has the shape 50,000 X 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 150)\n"
     ]
    }
   ],
   "source": [
    "np.save(\"./my_final_embeddings.npy\", final_embeddings)\n",
    "print(final_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Load the generated final_embeddings.npy file. Convert it to a pandas dataframe with 'vocabulary' as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "final_embeddings = np.load(\"./my_final_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UNK</th>\n",
       "      <td>-0.097174</td>\n",
       "      <td>-0.071319</td>\n",
       "      <td>0.026561</td>\n",
       "      <td>-0.058367</td>\n",
       "      <td>0.200439</td>\n",
       "      <td>0.008292</td>\n",
       "      <td>-0.066678</td>\n",
       "      <td>0.055755</td>\n",
       "      <td>0.027244</td>\n",
       "      <td>0.096656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046624</td>\n",
       "      <td>-0.045514</td>\n",
       "      <td>0.064631</td>\n",
       "      <td>0.122665</td>\n",
       "      <td>0.100420</td>\n",
       "      <td>0.017072</td>\n",
       "      <td>0.024613</td>\n",
       "      <td>0.063482</td>\n",
       "      <td>-0.100106</td>\n",
       "      <td>-0.056884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.063695</td>\n",
       "      <td>-0.006514</td>\n",
       "      <td>-0.095252</td>\n",
       "      <td>-0.126712</td>\n",
       "      <td>0.023827</td>\n",
       "      <td>-0.053248</td>\n",
       "      <td>-0.099918</td>\n",
       "      <td>0.107518</td>\n",
       "      <td>0.119890</td>\n",
       "      <td>0.114966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077067</td>\n",
       "      <td>-0.100760</td>\n",
       "      <td>-0.008488</td>\n",
       "      <td>0.059077</td>\n",
       "      <td>-0.013190</td>\n",
       "      <td>-0.000417</td>\n",
       "      <td>0.045537</td>\n",
       "      <td>0.094586</td>\n",
       "      <td>0.043825</td>\n",
       "      <td>-0.063683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.087955</td>\n",
       "      <td>-0.032489</td>\n",
       "      <td>-0.012846</td>\n",
       "      <td>-0.032974</td>\n",
       "      <td>0.078496</td>\n",
       "      <td>-0.038743</td>\n",
       "      <td>-0.029354</td>\n",
       "      <td>0.069618</td>\n",
       "      <td>0.089494</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075425</td>\n",
       "      <td>-0.149691</td>\n",
       "      <td>0.071689</td>\n",
       "      <td>0.037926</td>\n",
       "      <td>0.097742</td>\n",
       "      <td>0.028771</td>\n",
       "      <td>0.048178</td>\n",
       "      <td>0.080601</td>\n",
       "      <td>-0.071491</td>\n",
       "      <td>-0.030042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.098240</td>\n",
       "      <td>-0.083195</td>\n",
       "      <td>0.088678</td>\n",
       "      <td>-0.044420</td>\n",
       "      <td>0.078772</td>\n",
       "      <td>-0.049455</td>\n",
       "      <td>-0.058292</td>\n",
       "      <td>0.024770</td>\n",
       "      <td>0.084672</td>\n",
       "      <td>0.090246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098837</td>\n",
       "      <td>-0.043643</td>\n",
       "      <td>0.071639</td>\n",
       "      <td>0.086364</td>\n",
       "      <td>0.093321</td>\n",
       "      <td>0.072134</td>\n",
       "      <td>-0.002639</td>\n",
       "      <td>0.087253</td>\n",
       "      <td>-0.095189</td>\n",
       "      <td>-0.013743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.056014</td>\n",
       "      <td>-0.043074</td>\n",
       "      <td>-0.035271</td>\n",
       "      <td>-0.158378</td>\n",
       "      <td>0.088861</td>\n",
       "      <td>-0.064606</td>\n",
       "      <td>-0.058925</td>\n",
       "      <td>0.051635</td>\n",
       "      <td>0.077762</td>\n",
       "      <td>0.171583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098405</td>\n",
       "      <td>-0.078332</td>\n",
       "      <td>0.046792</td>\n",
       "      <td>0.122246</td>\n",
       "      <td>0.091336</td>\n",
       "      <td>0.024769</td>\n",
       "      <td>0.008617</td>\n",
       "      <td>0.025404</td>\n",
       "      <td>0.003028</td>\n",
       "      <td>0.000528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "UNK -0.097174 -0.071319  0.026561 -0.058367  0.200439  0.008292 -0.066678   \n",
       "the  0.063695 -0.006514 -0.095252 -0.126712  0.023827 -0.053248 -0.099918   \n",
       "of   0.087955 -0.032489 -0.012846 -0.032974  0.078496 -0.038743 -0.029354   \n",
       "and  0.098240 -0.083195  0.088678 -0.044420  0.078772 -0.049455 -0.058292   \n",
       "one  0.056014 -0.043074 -0.035271 -0.158378  0.088861 -0.064606 -0.058925   \n",
       "\n",
       "          7         8         9      ...          140       141       142  \\\n",
       "UNK  0.055755  0.027244  0.096656    ...     0.046624 -0.045514  0.064631   \n",
       "the  0.107518  0.119890  0.114966    ...     0.077067 -0.100760 -0.008488   \n",
       "of   0.069618  0.089494  0.112300    ...     0.075425 -0.149691  0.071689   \n",
       "and  0.024770  0.084672  0.090246    ...     0.098837 -0.043643  0.071639   \n",
       "one  0.051635  0.077762  0.171583    ...     0.098405 -0.078332  0.046792   \n",
       "\n",
       "          143       144       145       146       147       148       149  \n",
       "UNK  0.122665  0.100420  0.017072  0.024613  0.063482 -0.100106 -0.056884  \n",
       "the  0.059077 -0.013190 -0.000417  0.045537  0.094586  0.043825 -0.063683  \n",
       "of   0.037926  0.097742  0.028771  0.048178  0.080601 -0.071491 -0.030042  \n",
       "and  0.086364  0.093321  0.072134 -0.002639  0.087253 -0.095189 -0.013743  \n",
       "one  0.122246  0.091336  0.024769  0.008617  0.025404  0.003028  0.000528  \n",
       "\n",
       "[5 rows x 150 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "e = pd.DataFrame(final_embeddings, index=vocabulary)\n",
    "e.to_csv(r\"./vectors_new.txt\")\n",
    "e.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Print the first 10 coordinates of the vector representation of the words 'king', 'queen', 'man' and 'woman' from the 'vectors.txt' file generated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6221384  -2.381173   -0.62738276 -1.0975784   1.2373395  -3.1950328\n",
      " -0.74022526  0.47273895 -1.1066034  -1.5091859 ]\n",
      "[-1.8954808  -0.8628662  -2.0278444  -0.5545433   2.951458   -1.4985704\n",
      " -2.6483643  -2.0140924  -0.97132504 -1.605231  ]\n",
      "[-0.58591014 -1.0758336  -1.1054654  -2.7096047   0.16424485  0.6074105\n",
      " -0.13503285  2.3363764   3.0286949  -1.4967729 ]\n",
      "[ 1.4167926  -0.7298542  -2.2284133  -1.2658012   1.0197504  -0.02355143\n",
      "  1.0337201   0.6177318   1.4861876   0.45655224]\n"
     ]
    }
   ],
   "source": [
    "print(model.get_vector('queen')[0:10])\n",
    "print(model.get_vector('king')[0:10])\n",
    "print(model.get_vector('woman')[0:10])\n",
    "print(model.get_vector('man')[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Produce similar coordinates for these words from the embeddings dataframe created in Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.59343567e-03 -1.17386565e-01  4.20362763e-02 -8.97171646e-02\n",
      "  5.24427742e-02 -6.78752735e-02 -9.35444534e-02  1.11926205e-01\n",
      "  1.55509382e-01  1.54818146e-04  9.83842909e-02]\n",
      "[-0.0681529  -0.14757028 -0.10534908 -0.14574277 -0.03477092 -0.10491852\n",
      "  0.01287978 -0.0124207  -0.01159281  0.02592288  0.06745435]\n",
      "[-0.07151822  0.06574385 -0.04847812 -0.19753753 -0.06176797 -0.05839843\n",
      " -0.10179205 -0.03153234 -0.11444549 -0.04958469  0.01628827]\n",
      "[-0.02448057 -0.00275795  0.00944712  0.0062144  -0.04676778  0.01476785\n",
      " -0.01838603  0.13764898  0.04757234  0.03992737 -0.02046021]\n"
     ]
    }
   ],
   "source": [
    "print(e.loc['queen', 0:10].values)\n",
    "print(e.loc['king', 0:10].values)\n",
    "print(e.loc['woman', 0:10].values)\n",
    "print(e.loc['man', 0:10].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Problem 6 (Optional)}$\n",
    "At the very end of the provided notebook we run a t-SNE plot for\n",
    "four vectors: king, queen, man and woman and we were disappointed to discover that the\n",
    "advertised parallelism/equality between vectors 𝑣(‘𝑘𝑖𝑛𝑔’)– 𝑣(‘𝑞𝑢𝑒𝑒𝑛’) =\n",
    "𝑣(‘𝑚𝑎𝑛’) – 𝑣(‘𝑤𝑜𝑚𝑎𝑛’) is not there. SciKit-Learn has a PCA module as well. Please\n",
    "look at http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "Rather than plotting those 4 vectors using t-SNE, plot them using\n",
    "sklearn.decomposition.PCA. Examine the plot and tell us whether the advertised\n",
    "vector relationship is stronger this time. If you know how, you are welcome to use PCA\n",
    "tool in TensorBoard to present the same data.\n",
    "(25%) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Steps and Code:}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Find the first two principal components of the final_embedding matrix produced from the previous problem.<br>\n",
    "2\\. Transform the final_embedding matrix to the basis defined by these two principal components.<br>\n",
    "3\\. Plot the transformed final_embedding matrix for the rows corresponding to 'king', 'queen', 'man' and 'woman', using 'plot_with_labels' function. We dont see the advertised vector relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  #in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i,:]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x215c46cbf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "plot_only = 500\n",
    "low_dim_embs = pca.fit_transform(final_embeddings[:plot_only,:])\n",
    "#labels = [vocabulary[i] for i in range(plot_only,600)]\n",
    "\n",
    "labels = ['king','queen','man','woman']\n",
    "#labels = ['russia','england','norway','usa','turkey','moscow','london','washington','ankara','oslo']\n",
    "plot_with_labels(low_dim_embs, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
